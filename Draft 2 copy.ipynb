{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('sign_mnist_train.csv')\n",
    "test_data = pd.read_csv('sign_mnist_test.csv')\n",
    "\n",
    "# Extract labels and images\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# Remove 'J' (label 9) and 'Z' (label 25)\n",
    "valid_labels = [i for i in range(26) if i not in [9, 25]]\n",
    "train_mask = np.isin(y_train, valid_labels)\n",
    "test_mask = np.isin(y_test, valid_labels)\n",
    "\n",
    "X_train = X_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "X_test = X_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "# Adjust labels to be in range 0-23 instead of 0-24\n",
    "y_train = [i if i < 9 else i - 1 for i in y_train]\n",
    "y_test = [i if i < 9 else i - 1 for i in y_test]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Reshape images to 32x32 (since the images consists of 32x32 pixels)\n",
    "X_train = X_train.reshape(-1, 32, 32, 1)\n",
    "X_test = X_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create a fixed validation set and a test set from the testing data\n",
    "X_val, X_final_test, y_val, y_final_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_enc = to_categorical(y_train, num_classes=24)\n",
    "y_val_enc = to_categorical(y_val, num_classes=24)\n",
    "y_final_test_enc = to_categorical(y_final_test, num_classes=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 32, 32, 1)\n",
      "(7172, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2802 - loss: 2.3809 - val_accuracy: 0.6043 - val_loss: 1.2454\n",
      "Epoch 2/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.9229 - val_accuracy: 0.6511 - val_loss: 1.0503\n",
      "Epoch 3/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.4874 - val_accuracy: 0.7078 - val_loss: 0.8804\n",
      "Epoch 4/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9111 - loss: 0.2799 - val_accuracy: 0.7426 - val_loss: 0.8748\n",
      "Epoch 5/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9519 - loss: 0.1584 - val_accuracy: 0.6997 - val_loss: 1.1686\n",
      "Epoch 6/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9662 - loss: 0.1124 - val_accuracy: 0.7772 - val_loss: 0.9599\n",
      "Epoch 7/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1395 - val_accuracy: 0.7998 - val_loss: 0.9480\n",
      "Epoch 8/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0812 - val_accuracy: 0.7599 - val_loss: 1.1617\n",
      "Epoch 9/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0215 - val_accuracy: 0.7264 - val_loss: 1.0746\n",
      "Epoch 10/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1123 - val_accuracy: 0.8012 - val_loss: 0.9830\n",
      "Epoch 11/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0575 - val_accuracy: 0.7936 - val_loss: 1.0179\n",
      "Epoch 12/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0263 - val_accuracy: 0.8112 - val_loss: 0.9629\n",
      "Epoch 13/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.8190 - val_loss: 0.9979\n",
      "Epoch 14/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1132 - val_accuracy: 0.8115 - val_loss: 0.8870\n",
      "Epoch 15/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.8162 - val_loss: 0.9731\n",
      "Epoch 16/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0314 - val_accuracy: 0.7881 - val_loss: 0.9650\n",
      "Epoch 17/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0544 - val_accuracy: 0.7995 - val_loss: 0.9599\n",
      "Epoch 18/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0156 - val_accuracy: 0.7599 - val_loss: 1.0879\n",
      "Epoch 19/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0128 - val_accuracy: 0.8201 - val_loss: 1.0177\n",
      "Epoch 20/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0031 - val_accuracy: 0.6824 - val_loss: 1.6572\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Define the densely connected model\n",
    "dense_model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 1)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(24, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_dense = dense_model.fit(X_train, y_train_enc, epochs=20, validation_data=(X_val, y_val_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5699 - loss: 1.4749 - val_accuracy: 0.8667 - val_loss: 0.4074\n",
      "Epoch 2/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9978 - loss: 0.0216 - val_accuracy: 0.8846 - val_loss: 0.4054\n",
      "Epoch 3/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0025 - val_accuracy: 0.8673 - val_loss: 0.5328\n",
      "Epoch 4/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0209 - val_accuracy: 0.8938 - val_loss: 0.4102\n",
      "Epoch 5/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.1992e-04 - val_accuracy: 0.8918 - val_loss: 0.4420\n",
      "Epoch 6/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.8230e-04 - val_accuracy: 0.8985 - val_loss: 0.4202\n",
      "Epoch 7/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.1571e-04 - val_accuracy: 0.8963 - val_loss: 0.4635\n",
      "Epoch 8/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.1300e-05 - val_accuracy: 0.8926 - val_loss: 0.4730\n",
      "Epoch 9/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.8025e-05 - val_accuracy: 0.8957 - val_loss: 0.5014\n",
      "Epoch 10/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.8984e-05 - val_accuracy: 0.8974 - val_loss: 0.5104\n",
      "Epoch 11/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.9091 - val_loss: 0.4110\n",
      "Epoch 12/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.7109e-04 - val_accuracy: 0.9049 - val_loss: 0.4508\n",
      "Epoch 13/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 9.3961e-05 - val_accuracy: 0.9060 - val_loss: 0.4580\n",
      "Epoch 14/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.5413e-05 - val_accuracy: 0.8999 - val_loss: 0.4987\n",
      "Epoch 15/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.7255e-05 - val_accuracy: 0.8965 - val_loss: 0.5230\n",
      "Epoch 16/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.2827e-05 - val_accuracy: 0.8979 - val_loss: 0.5397\n",
      "Epoch 17/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.6482e-05 - val_accuracy: 0.8963 - val_loss: 0.5484\n",
      "Epoch 18/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0548e-05 - val_accuracy: 0.9041 - val_loss: 0.5605\n",
      "Epoch 19/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.6206e-06 - val_accuracy: 0.9035 - val_loss: 0.5744\n",
      "Epoch 20/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.3496e-06 - val_accuracy: 0.9021 - val_loss: 0.5958\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(24, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_cnn = cnn_model.fit(X_train, y_train_enc, epochs=20, validation_data=(X_val, y_val_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, X_final_test, y_final_test_enc, verbose = 2):\n",
    "    model_eval = model_name.evaluate(X_final_test, y_final_test_enc, verbose = verbose)\n",
    "    print(f\"Dense Model - Test Accuracy: {model_eval[1]}\")\n",
    "\n",
    "    # Detailed evaluation of the model\n",
    "    y_pred = np.argmax(model_name.predict(X_final_test), axis=1)\n",
    "\n",
    "    # Compute accuracy per class, skipping index 9 (for J)\n",
    "    accuracy_per_class = []\n",
    "    for i in range(24):\n",
    "        if np.sum(y_final_test == i) > 0:\n",
    "            accuracy_per_class.append(np.mean(y_pred[y_final_test == i] == i))\n",
    "        else:\n",
    "            accuracy_per_class.append(np.nan)  # Handle classes with no samples\n",
    "\n",
    "    # Filter out NaN values to calculate the median accuracy\n",
    "    valid_accuracies = [acc for acc in accuracy_per_class if not np.isnan(acc)]\n",
    "    median_accuracy = np.median(valid_accuracies)\n",
    "\n",
    "    print(f\"Unbiased Median Accuracy: {median_accuracy}\")\n",
    "\n",
    "    # Identify the letter with the highest individual accuracy\n",
    "    highest_accuracy_class = np.nanargmax(accuracy_per_class)\n",
    "    print(f\"Letter with Highest Accuracy: {chr(highest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "    # Identify the letter with the lowest individual accuracy\n",
    "    lowest_accuracy_class = np.nanargmin(accuracy_per_class)\n",
    "    print(f\"Letter with Lowest Accuracy: {chr(lowest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    conf_matrix = confusion_matrix(y_final_test, y_pred)\n",
    "\n",
    "    # Set the diagonal elements to zero to exclude correct classifications\n",
    "    np.fill_diagonal(conf_matrix, 0)\n",
    "\n",
    "    # Find the indices of the top three errors\n",
    "    errors = np.unravel_index(np.argsort(-conf_matrix, axis=None), conf_matrix.shape)\n",
    "\n",
    "    # Get the top three most common errors\n",
    "    common_errors = [(chr(errors[0][i] + ord('A')), chr(errors[1][i] + ord('A'))) for i in range(3)]\n",
    "    print(f\"Most Common Errors: {common_errors}\")\n",
    "\n",
    "    # Report overall mean accuracy and accuracy per letter\n",
    "    mean_accuracy = np.nanmean(accuracy_per_class)\n",
    "    print(f\"Overall Mean Accuracy: {mean_accuracy}\")\n",
    "\n",
    "    # Print each letter and its accuracy\n",
    "    letters = [chr(i + ord('A')) for i in range(26) if i not in [9, 25]]\n",
    "    for i, acc in enumerate(accuracy_per_class):\n",
    "        print(f\"Letter {letters[i]}: Accuracy {acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 - 0s - 836us/step - accuracy: 0.6938 - loss: 1.5337\n",
      "Dense Model - Test Accuracy: 0.6938092708587646\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step\n",
      "Unbiased Median Accuracy: 0.7361098285468033\n",
      "Letter with Highest Accuracy: K\n",
      "Letter with Lowest Accuracy: R\n",
      "Most Common Errors: [('N', 'S'), ('G', 'H'), ('L', 'I')]\n",
      "Overall Mean Accuracy: 0.6839723408333663\n",
      "Letter A: Accuracy 0.9878048780487805\n",
      "Letter B: Accuracy 0.751131221719457\n",
      "Letter C: Accuracy 0.8486842105263158\n",
      "Letter D: Accuracy 0.46511627906976744\n",
      "Letter E: Accuracy 0.8690476190476191\n",
      "Letter F: Accuracy 0.47692307692307695\n",
      "Letter G: Accuracy 0.7625\n",
      "Letter H: Accuracy 0.9248826291079812\n",
      "Letter I: Accuracy 0.7210884353741497\n",
      "Letter K: Accuracy 0.6481481481481481\n",
      "Letter L: Accuracy 1.0\n",
      "Letter M: Accuracy 0.45\n",
      "Letter N: Accuracy 0.45390070921985815\n",
      "Letter O: Accuracy 0.5041322314049587\n",
      "Letter P: Accuracy 0.88\n",
      "Letter Q: Accuracy 0.8133333333333334\n",
      "Letter R: Accuracy 0.8450704225352113\n",
      "Letter S: Accuracy 0.1864406779661017\n",
      "Letter T: Accuracy 0.6744186046511628\n",
      "Letter U: Accuracy 0.3235294117647059\n",
      "Letter V: Accuracy 0.6775956284153005\n",
      "Letter W: Accuracy 0.7572815533980582\n",
      "Letter X: Accuracy 0.6397058823529411\n",
      "Letter Y: Accuracy 0.754601226993865\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(dense_model, X_final_test, y_final_test_enc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 - 0s - 1ms/step - accuracy: 0.9186 - loss: 0.4880\n",
      "Dense Model - Test Accuracy: 0.9185722470283508\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Unbiased Median Accuracy: 0.9267629211132036\n",
      "Letter with Highest Accuracy: A\n",
      "Letter with Lowest Accuracy: Q\n",
      "Most Common Errors: [('L', 'R'), ('T', 'Q'), ('S', 'H')]\n",
      "Overall Mean Accuracy: 0.9094666290279649\n",
      "Letter A: Accuracy 1.0\n",
      "Letter B: Accuracy 1.0\n",
      "Letter C: Accuracy 0.9078947368421053\n",
      "Letter D: Accuracy 1.0\n",
      "Letter E: Accuracy 0.9563492063492064\n",
      "Letter F: Accuracy 1.0\n",
      "Letter G: Accuracy 0.95625\n",
      "Letter H: Accuracy 0.9859154929577465\n",
      "Letter I: Accuracy 0.9047619047619048\n",
      "Letter K: Accuracy 0.9382716049382716\n",
      "Letter L: Accuracy 1.0\n",
      "Letter M: Accuracy 0.8\n",
      "Letter N: Accuracy 0.8581560283687943\n",
      "Letter O: Accuracy 0.8842975206611571\n",
      "Letter P: Accuracy 1.0\n",
      "Letter Q: Accuracy 1.0\n",
      "Letter R: Accuracy 0.5211267605633803\n",
      "Letter S: Accuracy 0.9152542372881356\n",
      "Letter T: Accuracy 0.7286821705426356\n",
      "Letter U: Accuracy 0.8382352941176471\n",
      "Letter V: Accuracy 0.8907103825136612\n",
      "Letter W: Accuracy 1.0\n",
      "Letter X: Accuracy 0.8455882352941176\n",
      "Letter Y: Accuracy 0.8957055214723927\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(cnn_model, X_final_test, y_final_test_enc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,        # Randomly rotate images by 10 degrees\n",
    "    width_shift_range=0.1,    # Randomly translate images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,   # Randomly translate images vertically by 10% of the height\n",
    "    zoom_range=0.1,           # Randomly zoom images by 10%\n",
    "    horizontal_flip=True      # Randomly flip images horizontally\n",
    ")\n",
    "\n",
    "# Fit the generator to the training data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_dense_model_with_regularization(layer_sizes, learning_rate=0.001, l2_lambda=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(32, 32, 1)))\n",
    "    for size in layer_sizes:\n",
    "        model.add(Dense(size, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(24, activation='softmax', kernel_regularizer=l2(l2_lambda)))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_cnn_model_with_regularization(conv_layers, dense_layers, learning_rate=0.001, l2_lambda=0.01):\n",
    "    model = Sequential()\n",
    "    for filters, kernel_size in conv_layers:\n",
    "        model.add(Conv2D(filters, kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_lambda), input_shape=(32, 32, 1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    for size in dense_layers:\n",
    "        model.add(Dense(size, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(24, activation='softmax', kernel_regularizer=l2(l2_lambda)))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model: <Sequential name=sequential_14, built=True>\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 26/915\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0723 - loss: 13.5937    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.2309 - loss: 7.0087 - val_accuracy: 0.1277 - val_loss: 5.1259 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137us/step - accuracy: 0.4333 - loss: 2.3050 - val_accuracy: 0.1244 - val_loss: 5.0759 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m  1/915\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 39ms/step - accuracy: 0.2333 - loss: 2.8634"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3064 - loss: 2.6218 - val_accuracy: 0.1698 - val_loss: 5.0103 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.2667 - loss: 2.4667 - val_accuracy: 0.1743 - val_loss: 4.9178 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3849 - loss: 2.2518 - val_accuracy: 0.1194 - val_loss: 7.0874 - learning_rate: 0.0010\n",
      "Dense model: <Sequential name=sequential_15, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.2217 - loss: 10.8565 - val_accuracy: 0.0418 - val_loss: 7.7017 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194us/step - accuracy: 0.2000 - loss: 3.3610 - val_accuracy: 0.0569 - val_loss: 7.1467 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3034 - loss: 2.8370 - val_accuracy: 0.0477 - val_loss: 7.2845 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - accuracy: 0.3667 - loss: 2.4121 - val_accuracy: 0.0346 - val_loss: 8.0655 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3412 - loss: 2.5378 - val_accuracy: 0.0714 - val_loss: 5.2177 - learning_rate: 0.0010\n",
      "Dense model: <Sequential name=sequential_16, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.2066 - loss: 12.0202 - val_accuracy: 0.1311 - val_loss: 4.2244 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213us/step - accuracy: 0.2667 - loss: 3.0441 - val_accuracy: 0.1372 - val_loss: 4.0969 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.3212 - loss: 2.8325 - val_accuracy: 0.0647 - val_loss: 7.9125 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199us/step - accuracy: 0.3000 - loss: 2.6082 - val_accuracy: 0.0767 - val_loss: 7.2350 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.3478 - loss: 2.5980 - val_accuracy: 0.0429 - val_loss: 6.3200 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Define different dense models to experiment with\n",
    "dense_models = [\n",
    "    create_dense_model_with_regularization([512, 256], learning_rate=0.001),\n",
    "    create_dense_model_with_regularization([1024, 512, 256], learning_rate=0.001),\n",
    "    create_dense_model_with_regularization([1024, 512, 256, 128], learning_rate=0.001)\n",
    "]\n",
    "\n",
    "# Train and evaluate each dense model\n",
    "dense_histories = []\n",
    "for dense_model in dense_models:\n",
    "    print(f'Dense model: {dense_model}')\n",
    "    history = dense_model.fit(\n",
    "        datagen.flow(X_train, y_train_enc, batch_size=30),\n",
    "        steps_per_epoch=len(X_train) // 30,\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val_enc),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    dense_histories.append((val_accuracy, dense_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model: <Sequential name=sequential_11, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.5056 - loss: 3.5544 - val_accuracy: 0.7091 - val_loss: 1.6448 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - accuracy: 0.8667 - loss: 1.1777 - val_accuracy: 0.7119 - val_loss: 1.5918 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8417 - loss: 1.2361 - val_accuracy: 0.3960 - val_loss: 4.5198 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - accuracy: 0.8333 - loss: 1.0824 - val_accuracy: 0.4030 - val_loss: 4.5538 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8731 - loss: 1.0464 - val_accuracy: 0.7719 - val_loss: 1.2549 - learning_rate: 0.0010\n",
      "CNN model: <Sequential name=sequential_12, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5937 - loss: 4.8820 - val_accuracy: 0.7931 - val_loss: 1.4808 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.9333 - loss: 1.1664 - val_accuracy: 0.8006 - val_loss: 1.4715 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9027 - loss: 1.1355 - val_accuracy: 0.6433 - val_loss: 1.7131 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.8667 - loss: 1.1606 - val_accuracy: 0.5630 - val_loss: 1.9968 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9174 - loss: 0.9751 - val_accuracy: 0.8714 - val_loss: 1.0659 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - accuracy: 0.9333 - loss: 0.8019 - val_accuracy: 0.8742 - val_loss: 1.0494 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9252 - loss: 0.9067 - val_accuracy: 0.6001 - val_loss: 2.0468 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - accuracy: 0.8667 - loss: 0.9392 - val_accuracy: 0.6093 - val_loss: 1.9763 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9354 - loss: 0.8417 - val_accuracy: 0.8572 - val_loss: 1.0563 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - accuracy: 0.9333 - loss: 0.8649 - val_accuracy: 0.8661 - val_loss: 1.0307 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9761 - loss: 0.6176 - val_accuracy: 0.9964 - val_loss: 0.3575 - learning_rate: 2.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - accuracy: 1.0000 - loss: 0.3588 - val_accuracy: 0.9967 - val_loss: 0.3573 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9914 - loss: 0.3542 - val_accuracy: 0.9944 - val_loss: 0.2908 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.9667 - loss: 0.4202 - val_accuracy: 0.9922 - val_loss: 0.2944 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9909 - loss: 0.2828 - val_accuracy: 0.9975 - val_loss: 0.2399 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - accuracy: 1.0000 - loss: 0.2612 - val_accuracy: 0.9975 - val_loss: 0.2408 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9910 - loss: 0.2506 - val_accuracy: 0.9967 - val_loss: 0.2189 - learning_rate: 2.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.9667 - loss: 0.2490 - val_accuracy: 0.9969 - val_loss: 0.2188 - learning_rate: 2.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9917 - loss: 0.2314 - val_accuracy: 0.9953 - val_loss: 0.2018 - learning_rate: 2.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 1.0000 - loss: 0.1967 - val_accuracy: 0.9953 - val_loss: 0.2021 - learning_rate: 2.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9916 - loss: 0.2165 - val_accuracy: 0.9872 - val_loss: 0.2098 - learning_rate: 2.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 1.0000 - loss: 0.2272 - val_accuracy: 0.9872 - val_loss: 0.2097 - learning_rate: 2.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.1882 - val_accuracy: 0.9992 - val_loss: 0.1539 - learning_rate: 4.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 1.0000 - loss: 0.1556 - val_accuracy: 0.9992 - val_loss: 0.1539 - learning_rate: 4.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.1572 - val_accuracy: 0.9969 - val_loss: 0.1396 - learning_rate: 4.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 1.0000 - loss: 0.1610 - val_accuracy: 0.9969 - val_loss: 0.1396 - learning_rate: 4.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.1410 - val_accuracy: 1.0000 - val_loss: 0.1207 - learning_rate: 4.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - accuracy: 1.0000 - loss: 0.1270 - val_accuracy: 1.0000 - val_loss: 0.1207 - learning_rate: 4.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.1274 - val_accuracy: 0.9983 - val_loss: 0.1166 - learning_rate: 4.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - accuracy: 0.9667 - loss: 0.1615 - val_accuracy: 0.9983 - val_loss: 0.1165 - learning_rate: 4.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.1201 - val_accuracy: 0.9967 - val_loss: 0.1108 - learning_rate: 4.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 1.0000 - loss: 0.1155 - val_accuracy: 0.9967 - val_loss: 0.1109 - learning_rate: 4.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9987 - loss: 0.1123 - val_accuracy: 1.0000 - val_loss: 0.0983 - learning_rate: 4.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - accuracy: 1.0000 - loss: 0.1020 - val_accuracy: 1.0000 - val_loss: 0.0983 - learning_rate: 4.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.1068 - val_accuracy: 0.9997 - val_loss: 0.0947 - learning_rate: 4.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - accuracy: 1.0000 - loss: 0.0981 - val_accuracy: 0.9994 - val_loss: 0.0947 - learning_rate: 4.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.1035 - val_accuracy: 0.9994 - val_loss: 0.0892 - learning_rate: 4.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - accuracy: 1.0000 - loss: 0.1162 - val_accuracy: 0.9994 - val_loss: 0.0891 - learning_rate: 4.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0955 - val_accuracy: 0.9992 - val_loss: 0.0872 - learning_rate: 4.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - accuracy: 1.0000 - loss: 0.0901 - val_accuracy: 0.9989 - val_loss: 0.0877 - learning_rate: 4.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0945 - val_accuracy: 0.9992 - val_loss: 0.0817 - learning_rate: 4.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - accuracy: 1.0000 - loss: 0.0910 - val_accuracy: 0.9992 - val_loss: 0.0818 - learning_rate: 4.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0915 - val_accuracy: 0.9986 - val_loss: 0.0846 - learning_rate: 4.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - accuracy: 1.0000 - loss: 0.0889 - val_accuracy: 0.9986 - val_loss: 0.0841 - learning_rate: 4.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0855 - val_accuracy: 0.9997 - val_loss: 0.0765 - learning_rate: 1.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - accuracy: 1.0000 - loss: 0.0850 - val_accuracy: 0.9997 - val_loss: 0.0765 - learning_rate: 1.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0822 - val_accuracy: 1.0000 - val_loss: 0.0750 - learning_rate: 1.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - accuracy: 1.0000 - loss: 0.0883 - val_accuracy: 1.0000 - val_loss: 0.0750 - learning_rate: 1.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0808 - val_accuracy: 0.9997 - val_loss: 0.0737 - learning_rate: 1.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - accuracy: 1.0000 - loss: 0.0801 - val_accuracy: 0.9997 - val_loss: 0.0737 - learning_rate: 1.0000e-05\n",
      "CNN model: <Sequential name=sequential_13, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - accuracy: 0.5755 - loss: 7.3074 - val_accuracy: 0.8042 - val_loss: 1.7973 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - accuracy: 0.8667 - loss: 1.4850 - val_accuracy: 0.8148 - val_loss: 1.7783 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9051 - loss: 1.3622 - val_accuracy: 0.7197 - val_loss: 1.7858 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - accuracy: 0.9000 - loss: 1.2338 - val_accuracy: 0.7705 - val_loss: 1.6419 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.9228 - loss: 1.1350 - val_accuracy: 0.6308 - val_loss: 1.9861 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Define different CNN models with regularization to experiment with\n",
    "cnn_models = [\n",
    "    create_cnn_model_with_regularization([(32, (3, 3)), (64, (3, 3))], [128],learning_rate=0.001),\n",
    "    create_cnn_model_with_regularization([(32, (3, 3)), (64, (3, 3)), (128, (3, 3))], [256], learning_rate=0.001),\n",
    "    create_cnn_model_with_regularization([(32, (3, 3)), (64, (3, 3)), (128, (3, 3)), (256, (3, 3))], [512], learning_rate=0.001)\n",
    "]\n",
    "\n",
    "# Train and evaluate each CNN model\n",
    "cnn_histories = []\n",
    "for cnn_model in cnn_models:\n",
    "    print(f'CNN model: {cnn_model}')\n",
    "    history = cnn_model.fit(\n",
    "        datagen.flow(X_train, y_train_enc, batch_size=30),\n",
    "        steps_per_epoch=len(X_train) // 30,\n",
    "        epochs=50,\n",
    "        validation_data=(X_val, y_val_enc),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    cnn_histories.append((val_accuracy, cnn_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model - Final Test Accuracy: 0.9994422793388367\n"
     ]
    }
   ],
   "source": [
    "# Combine all histories and models\n",
    "all_histories = dense_histories + cnn_histories\n",
    "\n",
    "# Select the best model based on validation accuracy\n",
    "best_model = max(all_histories, key=lambda x: x[0])[1]\n",
    "\n",
    "# Evaluate the best model on the final test set\n",
    "final_eval = best_model.evaluate(X_final_test, y_final_test_enc, verbose=0)\n",
    "print(f\"Best Model - Final Test Accuracy: {final_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Unbiased Median Accuracy: 1.0\n",
      "Letter with Highest Accuracy: A\n",
      "Letter with Lowest Accuracy: B\n",
      "Most Common Errors: [('E', 'L'), ('B', 'T'), ('P', 'V')]\n",
      "Overall Mean Accuracy: 0.9996461191314133\n",
      "Letter A: Accuracy 1.0\n",
      "Letter B: Accuracy 0.995475113122172\n",
      "Letter C: Accuracy 1.0\n",
      "Letter D: Accuracy 1.0\n",
      "Letter E: Accuracy 0.996031746031746\n",
      "Letter F: Accuracy 1.0\n",
      "Letter G: Accuracy 1.0\n",
      "Letter H: Accuracy 1.0\n",
      "Letter I: Accuracy 1.0\n",
      "Letter K: Accuracy 1.0\n",
      "Letter L: Accuracy 1.0\n",
      "Letter M: Accuracy 1.0\n",
      "Letter N: Accuracy 1.0\n",
      "Letter O: Accuracy 1.0\n",
      "Letter P: Accuracy 1.0\n",
      "Letter Q: Accuracy 1.0\n",
      "Letter R: Accuracy 1.0\n",
      "Letter S: Accuracy 1.0\n",
      "Letter T: Accuracy 1.0\n",
      "Letter U: Accuracy 1.0\n",
      "Letter V: Accuracy 1.0\n",
      "Letter W: Accuracy 1.0\n",
      "Letter X: Accuracy 1.0\n",
      "Letter Y: Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict on the final test set\n",
    "y_pred = np.argmax(best_model.predict(X_final_test), axis=1)\n",
    "\n",
    "# Compute accuracy per class\n",
    "accuracy_per_class = []\n",
    "for i in range(24):\n",
    "    if np.sum(y_final_test == i) > 0:\n",
    "        accuracy = np.mean(y_pred[y_final_test == i] == i)\n",
    "        accuracy_per_class.append(accuracy)\n",
    "    else:\n",
    "        accuracy_per_class.append(np.nan)  # Handle classes with no samples\n",
    "\n",
    "# Filter out NaN values to calculate the median accuracy\n",
    "valid_accuracies = [acc for acc in accuracy_per_class if not np.isnan(acc)]\n",
    "median_accuracy = np.median(valid_accuracies)\n",
    "\n",
    "print(f\"Unbiased Median Accuracy: {median_accuracy}\")\n",
    "\n",
    "# Identify the letter with the highest individual accuracy\n",
    "highest_accuracy_class = np.nanargmax(accuracy_per_class)\n",
    "print(f\"Letter with Highest Accuracy: {chr(highest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "# Identify the letter with the lowest individual accuracy\n",
    "lowest_accuracy_class = np.nanargmin(accuracy_per_class)\n",
    "print(f\"Letter with Lowest Accuracy: {chr(lowest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_final_test, y_pred)\n",
    "\n",
    "# Set the diagonal elements to zero to exclude correct classifications\n",
    "np.fill_diagonal(conf_matrix, 0)\n",
    "\n",
    "# Find the indices of the top three errors\n",
    "errors = np.unravel_index(np.argsort(-conf_matrix, axis=None), conf_matrix.shape)\n",
    "\n",
    "# Get the top three most common errors\n",
    "common_errors = [(chr(errors[0][i] + ord('A')), chr(errors[1][i] + ord('A'))) for i in range(3)]\n",
    "print(f\"Most Common Errors: {common_errors}\")\n",
    "\n",
    "\n",
    "# Report overall mean accuracy and accuracy per letter\n",
    "mean_accuracy = np.nanmean(accuracy_per_class)\n",
    "print(f\"Overall Mean Accuracy: {mean_accuracy}\")\n",
    "\n",
    "# Print each letter and its accuracy\n",
    "letters = [chr(i + ord('A')) for i in range(26) if i not in [9, 25]]\n",
    "for i, acc in enumerate(accuracy_per_class):\n",
    "    print(f\"Letter {letters[i]}: Accuracy {acc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

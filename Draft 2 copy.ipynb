{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('sign_mnist_train.csv')\n",
    "test_data = pd.read_csv('sign_mnist_test.csv')\n",
    "\n",
    "# Extract labels and images\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust labels to be in range 0-23 instead of 0-24\n",
    "y_train = [i if i < 9 else i - 1 for i in y_train]\n",
    "y_test = [i if i < 9 else i - 1 for i in y_test]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Reshape images to 32x32 (since the images consists of 32x32 pixels)\n",
    "X_train = X_train.reshape(-1, 32, 32, 1)\n",
    "X_test = X_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Create a fixed validation set and a test set from the testing data\n",
    "X_val, X_final_test, y_val, y_final_test = train_test_split(X_test, y_test, stratify= y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_enc = to_categorical(y_train, num_classes=24)\n",
    "y_val_enc = to_categorical(y_val, num_classes=24)\n",
    "y_final_test_enc = to_categorical(y_final_test, num_classes=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 32, 32, 1)\n",
      "(7172, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2827 - loss: 2.3754 - val_accuracy: 0.5683 - val_loss: 1.3102\n",
      "Epoch 2/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.9249 - val_accuracy: 0.7192 - val_loss: 0.9155\n",
      "Epoch 3/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.5159 - val_accuracy: 0.7462 - val_loss: 0.8934\n",
      "Epoch 4/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9095 - loss: 0.2890 - val_accuracy: 0.7521 - val_loss: 0.8922\n",
      "Epoch 5/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.1866 - val_accuracy: 0.7513 - val_loss: 0.9831\n",
      "Epoch 6/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1147 - val_accuracy: 0.7607 - val_loss: 1.0020\n",
      "Epoch 7/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.1248 - val_accuracy: 0.7716 - val_loss: 0.9627\n",
      "Epoch 8/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0879 - val_accuracy: 0.8006 - val_loss: 0.9512\n",
      "Epoch 9/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0758 - val_accuracy: 0.8313 - val_loss: 0.8294\n",
      "Epoch 10/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0463 - val_accuracy: 0.6308 - val_loss: 2.1150\n",
      "Epoch 11/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1162 - val_accuracy: 0.7200 - val_loss: 1.4679\n",
      "Epoch 12/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.1372 - val_accuracy: 0.8226 - val_loss: 0.9335\n",
      "Epoch 13/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.8279 - val_loss: 0.9685\n",
      "Epoch 14/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0082 - val_accuracy: 0.7602 - val_loss: 0.9043\n",
      "Epoch 15/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0496 - val_accuracy: 0.7987 - val_loss: 0.8204\n",
      "Epoch 16/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0082 - val_accuracy: 0.8330 - val_loss: 0.8245\n",
      "Epoch 17/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0265 - val_accuracy: 0.7479 - val_loss: 1.1235\n",
      "Epoch 18/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0154 - val_accuracy: 0.8363 - val_loss: 0.8442\n",
      "Epoch 19/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0526 - val_accuracy: 0.8249 - val_loss: 0.8247\n",
      "Epoch 20/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0037 - val_accuracy: 0.8377 - val_loss: 0.8556\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "\n",
    "# Define the densely connected model\n",
    "dense_model = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 1)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(24, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "set_seed(42)\n",
    "dense_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_dense = dense_model.fit(X_train, y_train_enc, epochs=20, validation_data=(X_val, y_val_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5623 - loss: 1.5057 - val_accuracy: 0.8795 - val_loss: 0.4001\n",
      "Epoch 2/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0352 - val_accuracy: 0.9038 - val_loss: 0.3952\n",
      "Epoch 3/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0076 - val_accuracy: 0.9163 - val_loss: 0.3932\n",
      "Epoch 4/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9200 - val_loss: 0.3761\n",
      "Epoch 5/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.0789e-04 - val_accuracy: 0.9211 - val_loss: 0.3821\n",
      "Epoch 6/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.3329e-04 - val_accuracy: 0.9214 - val_loss: 0.3918\n",
      "Epoch 7/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.4209e-04 - val_accuracy: 0.9208 - val_loss: 0.4043\n",
      "Epoch 8/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.7786e-05 - val_accuracy: 0.9194 - val_loss: 0.4175\n",
      "Epoch 9/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.4422e-05 - val_accuracy: 0.9197 - val_loss: 0.4305\n",
      "Epoch 10/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.3674e-05 - val_accuracy: 0.9194 - val_loss: 0.4465\n",
      "Epoch 11/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0749e-05 - val_accuracy: 0.9183 - val_loss: 0.4620\n",
      "Epoch 12/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.2772e-05 - val_accuracy: 0.9186 - val_loss: 0.4781\n",
      "Epoch 13/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.9164e-06 - val_accuracy: 0.9169 - val_loss: 0.4961\n",
      "Epoch 14/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.8694e-06 - val_accuracy: 0.9175 - val_loss: 0.5105\n",
      "Epoch 15/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.9861e-06 - val_accuracy: 0.9172 - val_loss: 0.5286\n",
      "Epoch 16/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.8445e-06 - val_accuracy: 0.9163 - val_loss: 0.5432\n",
      "Epoch 17/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.1474e-06 - val_accuracy: 0.9177 - val_loss: 0.5600\n",
      "Epoch 18/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.0926e-07 - val_accuracy: 0.9183 - val_loss: 0.5747\n",
      "Epoch 19/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.4490e-07 - val_accuracy: 0.9191 - val_loss: 0.5890\n",
      "Epoch 20/20\n",
      "\u001b[1m858/858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.7687e-07 - val_accuracy: 0.9194 - val_loss: 0.6062\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(24, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "set_seed(42)\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_cnn = cnn_model.fit(X_train, y_train_enc, epochs=20, validation_data=(X_val, y_val_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def evaluate_model(name, model_name, X_final_test, y_final_test_enc, verbose = 2):\n",
    "    model_eval = model_name.evaluate(X_final_test, y_final_test_enc, verbose = verbose)\n",
    "    print(f\"{name} Model - Test Accuracy: {model_eval[1]}\")\n",
    "\n",
    "    # Detailed evaluation of the model\n",
    "    y_pred = np.argmax(model_name.predict(X_final_test), axis=1)\n",
    "\n",
    "    # Compute accuracy per class, skipping index 9 (for J)\n",
    "    y_true = np.argmax(y_final_test_enc, axis=1)\n",
    "    accuracy_per_class = []\n",
    "    for i in range(24):\n",
    "        if np.sum(y_true == i) > 0:\n",
    "            accuracy_per_class.append(np.mean(y_pred[y_true == i] == i))\n",
    "        else:\n",
    "            accuracy_per_class.append(np.nan)  # Handle classes with no samples\n",
    "\n",
    "    # Filter out NaN values to calculate the median accuracy\n",
    "    valid_accuracies = [acc for acc in accuracy_per_class if not np.isnan(acc)]\n",
    "    median_accuracy = np.median(valid_accuracies)\n",
    "\n",
    "    print(f\"Unbiased Median Accuracy: {median_accuracy}\")\n",
    "\n",
    "    # Identify the letter with the highest individual accuracy\n",
    "    highest_accuracy_class = np.nanargmax(accuracy_per_class)\n",
    "    print(f\"Letter with Highest Accuracy: {chr(highest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "    # Identify the letter with the lowest individual accuracy\n",
    "    lowest_accuracy_class = np.nanargmin(accuracy_per_class)\n",
    "    print(f\"Letter with Lowest Accuracy: {chr(lowest_accuracy_class + ord('A'))}\")\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "  \n",
    "    conf_matrix = confusion_matrix(y_final_test, y_pred)\n",
    "\n",
    "    # Set the diagonal elements to zero to exclude correct classifications\n",
    "    np.fill_diagonal(conf_matrix, 0)\n",
    "\n",
    "    # Find the indices of the top three errors\n",
    "    errors = np.unravel_index(np.argsort(-conf_matrix, axis=None), conf_matrix.shape)\n",
    "\n",
    "    # Get the top three most common errors\n",
    "    common_errors = [(chr(errors[0][i] + ord('A')), chr(errors[1][i] + ord('A'))) for i in range(3)]\n",
    "    print(f\"Most Common Errors: {common_errors}\")\n",
    "\n",
    "    # Report overall mean accuracy and accuracy per letter\n",
    "    mean_accuracy = np.nanmean(accuracy_per_class)\n",
    "    print(f\"Overall Mean Accuracy: {mean_accuracy}\")\n",
    "\n",
    "    # Print each letter and its accuracy\n",
    "    letters = [chr(i + ord('A')) for i in range(26) if i not in [9, 25]]\n",
    "    for i, acc in enumerate(accuracy_per_class):\n",
    "        print(f\"Letter {letters[i]}: Accuracy {acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,        # Randomly rotate images by 10 degrees\n",
    "    width_shift_range=0.1,    # Randomly translate images horizontally by 10% of the width\n",
    "    height_shift_range=0.1,   # Randomly translate images vertically by 10% of the height\n",
    "    zoom_range=0.1,           # Randomly zoom images by 10%\n",
    "    horizontal_flip=True      # Randomly flip images horizontally\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_dense_model_with_regularization(layer_sizes, learning_rate=0.001, l2_lambda=0.01):\n",
    "    set_seed(42)\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(32, 32, 1)))\n",
    "    for size in layer_sizes:\n",
    "        model.add(Dense(size, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(24, activation='softmax', kernel_regularizer=l2(l2_lambda)))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "\n",
    "# def create_cnn_model_with_regularization(conv_layers, dense_layers, learning_rate=0.1, l2_lambda=0.01):\n",
    "#     model = Sequential()\n",
    "#     # model = keras.models.Sequential()\n",
    "#     for filters, kernel_size in conv_layers:\n",
    "#         model.add(Conv2D(filters, kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_lambda), input_shape=(32, 32, 1)))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "#     model.add(Flatten())\n",
    "#     for size in dense_layers:\n",
    "#         model.add(Dense(size, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "#         model.add(BatchNormalization())\n",
    "#     model.add(Dense(24, activation='softmax', kernel_regularizer=l2(l2_lambda)))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "def create_cnn_model_with_regularization(conv_layers, dense_layers, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    # model = keras.models.Sequential()\n",
    "    for filters, kernel_size in conv_layers:\n",
    "        model.add(Conv2D(filters, kernel_size, activation='relu', input_shape=(32, 32, 1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    for size in dense_layers:\n",
    "        model.add(Dense(size, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(24, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_best_model(group_name, model_group, X_train, y_train_enc, batch_size = 30):\n",
    "    # Train and evaluate each  model\n",
    "    model_histories = []\n",
    "    for model in model_group:\n",
    "        print(f'Model for {group_name}: {model}')\n",
    "        set_seed(42)\n",
    "        history = model.fit(\n",
    "            datagen.flow(X_train, y_train_enc, batch_size=batch_size),\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            epochs=50,\n",
    "            validation_data=(X_val, y_val_enc),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        model_histories.append((val_accuracy, model))\n",
    "\n",
    "    # Determine the best model based on validation accuracy\n",
    "    best_val_accuracy, best_model = max(model_histories, key=lambda item: item[0])\n",
    "\n",
    "    print(f'The best model is: {best_model} with a validation accuracy of: {best_val_accuracy}')\n",
    "    return model_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different dense models to experiment with\n",
    "dense_models = [\n",
    "    create_dense_model_with_regularization([512, 256], learning_rate=0.001),\n",
    "    create_dense_model_with_regularization([1024, 512, 256], learning_rate=0.001),\n",
    "    create_dense_model_with_regularization([1024, 512, 256, 128], learning_rate=0.001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different CNN models with regularization to experiment with\n",
    "cnn_models = [\n",
    "    create_cnn_model_with_regularization([(32, (3, 3)), (64, (3, 3))], [128],learning_rate=0.001),\n",
    "    create_cnn_model_with_regularization([(32, (3, 3)), (64, (3, 3)), (128, (3, 3))], [256], learning_rate=0.001),\n",
    "    create_cnn_model_with_regularization([(16, (3,3)), (32, (3, 3)), (64, (3, 3))], [512, 128], learning_rate=0.01)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Dense: <Sequential name=sequential_10, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m 25/915\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1150 - loss: 13.4553    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.2234 - loss: 6.9141 - val_accuracy: 0.0449 - val_loss: 11.3997 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140us/step - accuracy: 0.2667 - loss: 2.8160 - val_accuracy: 0.0410 - val_loss: 10.8091 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m  1/915\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 40ms/step - accuracy: 0.3667 - loss: 2.3469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3164 - loss: 2.5573 - val_accuracy: 0.0678 - val_loss: 7.4718 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137us/step - accuracy: 0.5000 - loss: 2.1178 - val_accuracy: 0.0906 - val_loss: 7.2335 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3567 - loss: 2.3558 - val_accuracy: 0.2412 - val_loss: 3.1690 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137us/step - accuracy: 0.5667 - loss: 1.9313 - val_accuracy: 0.2496 - val_loss: 3.1776 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3763 - loss: 2.2212 - val_accuracy: 0.0982 - val_loss: 5.0094 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134us/step - accuracy: 0.4667 - loss: 2.0363 - val_accuracy: 0.1135 - val_loss: 4.3714 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4357 - loss: 2.0056 - val_accuracy: 0.4074 - val_loss: 1.9742 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135us/step - accuracy: 0.4333 - loss: 1.9372 - val_accuracy: 0.4219 - val_loss: 1.9403 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4596 - loss: 1.9114 - val_accuracy: 0.3776 - val_loss: 2.0926 - learning_rate: 2.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.4333 - loss: 2.0625 - val_accuracy: 0.3862 - val_loss: 2.0356 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4782 - loss: 1.8665 - val_accuracy: 0.4267 - val_loss: 1.8359 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135us/step - accuracy: 0.7333 - loss: 1.5139 - val_accuracy: 0.4573 - val_loss: 1.7739 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5052 - loss: 1.7980 - val_accuracy: 0.4417 - val_loss: 1.8632 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137us/step - accuracy: 0.5667 - loss: 1.6713 - val_accuracy: 0.4345 - val_loss: 1.8992 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5256 - loss: 1.7423 - val_accuracy: 0.3224 - val_loss: 2.4576 - learning_rate: 2.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.4667 - loss: 1.6267 - val_accuracy: 0.3282 - val_loss: 2.4270 - learning_rate: 4.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5344 - loss: 1.7299 - val_accuracy: 0.6127 - val_loss: 1.4427 - learning_rate: 4.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.6333 - loss: 1.6998 - val_accuracy: 0.6093 - val_loss: 1.4464 - learning_rate: 4.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5516 - loss: 1.6773 - val_accuracy: 0.6378 - val_loss: 1.3652 - learning_rate: 4.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139us/step - accuracy: 0.5667 - loss: 1.5304 - val_accuracy: 0.6352 - val_loss: 1.3686 - learning_rate: 4.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5657 - loss: 1.6316 - val_accuracy: 0.6118 - val_loss: 1.4501 - learning_rate: 4.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140us/step - accuracy: 0.6333 - loss: 1.5529 - val_accuracy: 0.6138 - val_loss: 1.4452 - learning_rate: 4.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5796 - loss: 1.5951 - val_accuracy: 0.6668 - val_loss: 1.3362 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137us/step - accuracy: 0.5333 - loss: 1.6526 - val_accuracy: 0.6684 - val_loss: 1.3358 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5771 - loss: 1.5952 - val_accuracy: 0.6601 - val_loss: 1.3078 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144us/step - accuracy: 0.6000 - loss: 1.4144 - val_accuracy: 0.6617 - val_loss: 1.3073 - learning_rate: 1.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5757 - loss: 1.6012 - val_accuracy: 0.6721 - val_loss: 1.3143 - learning_rate: 1.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140us/step - accuracy: 0.5667 - loss: 1.5230 - val_accuracy: 0.6729 - val_loss: 1.3140 - learning_rate: 1.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5810 - loss: 1.5897 - val_accuracy: 0.6704 - val_loss: 1.3163 - learning_rate: 1.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138us/step - accuracy: 0.5667 - loss: 1.6236 - val_accuracy: 0.6695 - val_loss: 1.3160 - learning_rate: 1.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5788 - loss: 1.5889 - val_accuracy: 0.6642 - val_loss: 1.3009 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.7000 - loss: 1.3637 - val_accuracy: 0.6631 - val_loss: 1.3026 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5834 - loss: 1.5830 - val_accuracy: 0.6718 - val_loss: 1.3062 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139us/step - accuracy: 0.6333 - loss: 1.4967 - val_accuracy: 0.6729 - val_loss: 1.3061 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5857 - loss: 1.5825 - val_accuracy: 0.6475 - val_loss: 1.3929 - learning_rate: 1.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136us/step - accuracy: 0.6333 - loss: 1.4180 - val_accuracy: 0.6470 - val_loss: 1.3926 - learning_rate: 1.0000e-05\n",
      "Model for Dense: <Sequential name=sequential_11, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.2177 - loss: 10.6784 - val_accuracy: 0.0229 - val_loss: 12.8680 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196us/step - accuracy: 0.2000 - loss: 2.8460 - val_accuracy: 0.0229 - val_loss: 13.1950 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.3396 - loss: 2.7291 - val_accuracy: 0.0887 - val_loss: 5.3439 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183us/step - accuracy: 0.4333 - loss: 2.3786 - val_accuracy: 0.0806 - val_loss: 5.6529 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3784 - loss: 2.4757 - val_accuracy: 0.1051 - val_loss: 6.6413 - learning_rate: 0.0010\n",
      "Model for Dense: <Sequential name=sequential_12, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.2051 - loss: 12.1005 - val_accuracy: 0.0229 - val_loss: 12.4115 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206us/step - accuracy: 0.2000 - loss: 3.1185 - val_accuracy: 0.0229 - val_loss: 12.1425 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.3088 - loss: 2.9155 - val_accuracy: 0.0856 - val_loss: 5.0541 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188us/step - accuracy: 0.3667 - loss: 2.4952 - val_accuracy: 0.0839 - val_loss: 5.2258 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.3287 - loss: 2.6788 - val_accuracy: 0.0694 - val_loss: 5.3660 - learning_rate: 0.0010\n",
      "The best model is: <Sequential name=sequential_10, built=True> with a validation accuracy of: 0.6469603776931763\n"
     ]
    }
   ],
   "source": [
    "dense_histories = return_best_model(\"Dense\", dense_models, X_train, y_train_enc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for CNN: <Sequential name=sequential_13, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m 27/915\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9953 - loss: 0.0211 - val_accuracy: 0.9992 - val_loss: 0.0085 - learning_rate: 1.0000e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 0.9992 - val_loss: 0.0085 - learning_rate: 1.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linhn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9967 - loss: 0.0178 - val_accuracy: 0.9989 - val_loss: 0.0086 - learning_rate: 1.0000e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9992 - val_loss: 0.0086 - learning_rate: 1.0000e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9974 - loss: 0.0138 - val_accuracy: 0.9997 - val_loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.9997 - val_loss: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0110 - val_accuracy: 0.9997 - val_loss: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 0.9997 - val_loss: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0084 - val_accuracy: 0.9997 - val_loss: 0.0070 - learning_rate: 1.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step - accuracy: 1.0000 - loss: 0.0149 - val_accuracy: 0.9997 - val_loss: 0.0070 - learning_rate: 1.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0111 - val_accuracy: 0.9997 - val_loss: 0.0070 - learning_rate: 1.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.9997 - val_loss: 0.0070 - learning_rate: 1.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0092 - val_accuracy: 0.9997 - val_loss: 0.0073 - learning_rate: 1.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222us/step - accuracy: 0.9667 - loss: 0.0537 - val_accuracy: 0.9997 - val_loss: 0.0073 - learning_rate: 1.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0155 - val_accuracy: 0.9994 - val_loss: 0.0063 - learning_rate: 1.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0063 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9979 - loss: 0.0146 - val_accuracy: 0.9994 - val_loss: 0.0068 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.9994 - val_loss: 0.0068 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9964 - loss: 0.0163 - val_accuracy: 0.9994 - val_loss: 0.0059 - learning_rate: 1.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.9994 - val_loss: 0.0059 - learning_rate: 1.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9955 - loss: 0.0201 - val_accuracy: 0.9997 - val_loss: 0.0061 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 0.9997 - val_loss: 0.0062 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9960 - loss: 0.0181 - val_accuracy: 0.9997 - val_loss: 0.0059 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225us/step - accuracy: 0.9667 - loss: 0.0495 - val_accuracy: 0.9997 - val_loss: 0.0059 - learning_rate: 1.0000e-05\n",
      "Model for CNN: <Sequential name=sequential_14, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9606 - loss: 0.1396 - val_accuracy: 0.9788 - val_loss: 0.0703 - learning_rate: 2.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - accuracy: 1.0000 - loss: 0.0484 - val_accuracy: 0.9799 - val_loss: 0.0665 - learning_rate: 2.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9731 - loss: 0.0951 - val_accuracy: 0.9587 - val_loss: 0.1478 - learning_rate: 2.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274us/step - accuracy: 1.0000 - loss: 0.0474 - val_accuracy: 0.9587 - val_loss: 0.1467 - learning_rate: 2.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9826 - loss: 0.0604 - val_accuracy: 0.9847 - val_loss: 0.0639 - learning_rate: 2.0000e-04\n",
      "Model for CNN: <Sequential name=sequential_15, built=True>\n",
      "Epoch 1/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8164 - loss: 0.5430 - val_accuracy: 0.8026 - val_loss: 0.6864 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - accuracy: 0.9333 - loss: 0.1991 - val_accuracy: 0.7616 - val_loss: 0.8457 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8961 - loss: 0.3147 - val_accuracy: 0.9504 - val_loss: 0.1565 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - accuracy: 0.9000 - loss: 0.1582 - val_accuracy: 0.9473 - val_loss: 0.1688 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m915/915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9296 - loss: 0.2141 - val_accuracy: 0.9292 - val_loss: 0.2443 - learning_rate: 0.0100\n",
      "The best model is: <Sequential name=sequential_13, built=True> with a validation accuracy of: 0.999721109867096\n"
     ]
    }
   ],
   "source": [
    "cnn_histories = return_best_model(\"CNN\", cnn_models, X_train, y_train_enc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: <Sequential name=sequential_13, built=True> - Final Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Combine all histories and models\n",
    "all_histories = dense_histories + cnn_histories\n",
    "\n",
    "# Select the best model based on validation accuracy\n",
    "best_model = max(all_histories, key=lambda x: x[0])[1]\n",
    "\n",
    "\n",
    "# Evaluate the best model on the final test set\n",
    "final_eval = best_model.evaluate(X_final_test, y_final_test_enc, verbose=0)\n",
    "print(f\"Best Model: {best_model} - Final Test Accuracy: {final_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 - 0s - 2ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "CNN Model - Test Accuracy: 1.0\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Unbiased Median Accuracy: 1.0\n",
      "Letter with Highest Accuracy: A\n",
      "Letter with Lowest Accuracy: A\n",
      "Most Common Errors: [('A', 'A'), ('P', 'U'), ('P', 'V')]\n",
      "Overall Mean Accuracy: 1.0\n",
      "Letter A: Accuracy 1.0\n",
      "Letter B: Accuracy 1.0\n",
      "Letter C: Accuracy 1.0\n",
      "Letter D: Accuracy 1.0\n",
      "Letter E: Accuracy 1.0\n",
      "Letter F: Accuracy 1.0\n",
      "Letter G: Accuracy 1.0\n",
      "Letter H: Accuracy 1.0\n",
      "Letter I: Accuracy 1.0\n",
      "Letter K: Accuracy 1.0\n",
      "Letter L: Accuracy 1.0\n",
      "Letter M: Accuracy 1.0\n",
      "Letter N: Accuracy 1.0\n",
      "Letter O: Accuracy 1.0\n",
      "Letter P: Accuracy 1.0\n",
      "Letter Q: Accuracy 1.0\n",
      "Letter R: Accuracy 1.0\n",
      "Letter S: Accuracy 1.0\n",
      "Letter T: Accuracy 1.0\n",
      "Letter U: Accuracy 1.0\n",
      "Letter V: Accuracy 1.0\n",
      "Letter W: Accuracy 1.0\n",
      "Letter X: Accuracy 1.0\n",
      "Letter Y: Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\"CNN\", best_model, X_final_test, y_final_test_enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
